[
  {
    "objectID": "projecttwo.html",
    "href": "projecttwo.html",
    "title": "Analysis using Netflix Descriptions, genres, and actors.",
    "section": "",
    "text": "My overall goal is to analyze this Netflix dataset is to: 1. Detect a word in a description, such as family, love, and friend, and compare how many times it appears in certain genres and compare it with others. 2. See how long descriptions are in comparison to movies/shows that have actors that start with T or A followed with a vowel to those that don’t.\nHopefully these graphs can show something useful but due to this dataset being only around ~8000 rows and my observations being very specific may lead to little to no results."
  },
  {
    "objectID": "projecttwo.html#final-visuals",
    "href": "projecttwo.html#final-visuals",
    "title": "Analysis using Netflix Descriptions, genres, and actors.",
    "section": "Final Visuals",
    "text": "Final Visuals\nI created three graphs located below answering my questions at the beginning.\n\nCount of Word in Netflix Movie Descriptions by Genre\nIn this data set, about movies, we can see that the word “love” is mentioned the most in the description of comedies and least in Action & Adventure. Friend is mentioned the same amount of times throughout the three genres. Family is mentioned the most in Comedies and Dramas and the least in comedies.\n\nlibrary(ggplot2)\nlibrary(tidyr)\nlibrary(dplyr)\nggplot(netflix_movies_filter, aes(fill = keyword, y = count, x = listed_in)) + \n  geom_bar(position = \"dodge\", stat = \"identity\") +  \n  labs(title = \"Count of Word in Netflix Movie Descriptions by Genre\",\n       x = \"Genre\",\n       y = \"Count of Keywords\",\n       fill = \"Keyword\") +  \n  theme_minimal()  \n\n\n\n\n\n\n\n\n\n\nCount of Word in Netflix Shows Descriptions by Genre\nIn this dataset, about shows, the most insightful thing we see is that love is mentioned the most in Dramas and comedies in comparison to Action & Adventure.\n\nlibrary(ggplot2)\nlibrary(tidyr)\nlibrary(dplyr)\nggplot(netflix_shows_filter, aes(fill = keyword, y = count, x = listed_in)) + \n  geom_bar(position = \"dodge\", stat = \"identity\") +  # Grouped bars\n  labs(title = \"Count of Word in Netflix TV Shows Descriptions by Genre\",\n       x = \"Genre\",\n       y = \"Count of Keywords\",\n       fill = \"Keyword\") + \n  theme_minimal()  \n\n\n\n\n\n\n\n\n\n\nDescription Length in Comparison with Actors starting with T or A\nSince the boxplot isn’t that different between each other, we can make grand observation. One thing to note is that cast labeled as N/A has very short descriptions.\n\nlibrary(ggplot2)\n\nggplot(netflix_titles, aes(x = as.factor(actor_TA), y = description_length, fill = as.factor(actor_TA))) +\n  geom_boxplot() +\n  labs(title = \"Description Length in Comparison with Actors starting with T or A\",\n       x = \"Contains 'T' or 'A' in Cast\",\n       y = \"Description Length\",\n       fill = \"Contains 'T' or 'A' \") +\n  scale_x_discrete(labels = c(\"No\", \"Yes\")) + \n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nReflection\nOverall, I feel like I can gain more insight if I use multiple words and more genres. The word “love”, “family”, and “friend” is very specific and that makes it so our histograms are so small. It would have been better if I used more generic words or used a variety of words. On the counterpart, when looking at actors that start with T or A, I need to be more specific so I can see a difference between the two groups."
  },
  {
    "objectID": "projecttwo.html#data-background",
    "href": "projecttwo.html#data-background",
    "title": "Analysis using Netflix Descriptions, genres, and actors.",
    "section": "Data Background",
    "text": "Data Background\nOriginally, before being inputted into TidyTuesay, this data can be found from Kaggle by the user Shivam Bansal who uploaded it 3 years ago. Found on this website: https://www.kaggle.com/datasets/shivamb/netflix-shows\nThis data has had for 500k downloads, and 3 million views from users. No purpose or origin is given as to why this data was even collected so it appears as if this user just gathered this data for the public to use as it was accessed through a public domain. The user has done the same for other websites such as Amazon Prime Video, disney+ and Hulu. This is what the author commented on the description of the dataset:\n“Netflix is one of the most popular media and video streaming platforms. They have over 8000 movies or tv shows available on their platform, as of mid-2021, they have over 200M Subscribers globally. This tabular dataset consists of listings of all the movies and tv shows available on Netflix, along with details such as - cast, directors, ratings, release year, duration, etc.” ### Reference\nI got my data set from the tidy tuesday website/section on github. Found at:\nhttps://github.com/rfordatascience/tidytuesday/blob/master/data/2021/2021-04-20/readme.md"
  },
  {
    "objectID": "project3.html",
    "href": "project3.html",
    "title": "A Simulation study: Chance of Winning Blackjack",
    "section": "",
    "text": "Description of Project\nEven though I just turned 20 this October, I believe a lot of people are complexed by casino games and its probabilities when it comes to winning. How likely are you to win at a game of poker, a game of roulette, or even a game of blackjack? Are any of these games skill based or in “your” favor? What game should I go and play if I want to earn money fast? Now I am not going to go gamble any time soon, not only because I am underage but also know better then to fall in that rabbit hole, but I really want to learn more about probabilities when it comes to playing such games and putting one’s money the line. I have heard a lot about the game blackjack and would want to test the probability of getting a value higher than or equal to 17 when drawing two cards because apparently you don’t want to hit if you are higher than 17. I am going to get this probability by running 10,000 simulations of 1)creating a deck with all amounts of cards, 2) have a variable have randomly chosen two values from such deck and 3) getting a logical value if the sum is between 17 and 21 (true) or not (false). I will also vary the amount of cards someone draws to see a range of probabilities\n\n\nCode\n1st step: Creates a deck containing 4 values of 2-10, 4 values of 10, and 4 values of 11. Problem: Can’t tell which are diamonds, spades, clovers, or hearts however in this instance we only care about the value so this works. The deck is also irreplacable when you take a card out.\n\nlibrary(tidyverse)\n\nnewdeck &lt;- function() {\n  deck &lt;- c(rep(2:10, each = 4), rep(10, 4*3), rep(11, 4))  \n  return(deck)\n}\n\n\ndeck &lt;- newdeck()\ndeck\n\n [1]  2  2  2  2  3  3  3  3  4  4  4  4  5  5  5  5  6  6  6  6  7  7  7  7  8\n[26]  8  8  8  9  9  9  9 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 11 11\n[51] 11 11\n\n\n\nAdd values we drew, also has a while loop that changes aces from 11 to 1 if we are above 21.\n\n\ncalculate_value &lt;- function(n) {\n  total &lt;- sum(n)\n  num_aces &lt;- sum(n == 11)\n  while(total &gt; 21 && num_aces &gt;0){\n    total &lt;- total - 10\n    num_aces &lt;- num_aces -1\n  }\n  return(total)\n}\n\n\nRuns one simulation of drawing two cards from the deck and returns true or false if it is a high value (17-21)\n\n\nbetween17and21 &lt;- function(num_cards) {\n  deck &lt;- newdeck()\n  hand &lt;- sample(deck, num_cards, replace = FALSE)\n  hand_value &lt;- calculate_value(hand)\n  return(hand_value &gt;= 17 && hand_value &lt;= 21)\n}\n\n4.Run simulation multiple times and right now we are only testing 2 cards drawn.\n\ndraw_cards &lt;- function(nums,reps){\n  probability &lt;- map_lgl(1:reps, ~between17and21(nums)) |&gt; mean()\n  return(data.frame(high_hand_prob = probability, amount_of_cards = nums))\n}\n\n\nset.seed(47)\ndraw_cards(2,10000)\n\n  high_hand_prob amount_of_cards\n1         0.3474               2\n\n\n\n\nVisualizations\nIterate it from drawing 2 to 10 cards and Visualization: This graph represents a line graph of probabilities of a hand value being between 17-21 based on the amount of cards we draw. It has a downward trend which makes sense however we can see that at 2-3 cards we have a probability around 30% which is pretty high considering that we want to be in such a high range when playing this game. The cut off is around 6 cards where it is almost impossible to be at such a value.\n\nset.seed(47)\nmap(2:10, ~draw_cards(.x, reps = 10000)) |&gt; \n  list_rbind()|&gt;\n  ggplot(aes(x= amount_of_cards, y = high_hand_prob)) +\n  geom_line() +\n  labs(title = \"Probability of having a high hand value (17-21) in BlackJack\" ,y = \"probability of having a hand value of 17-21\", x = \"number of cards drawn\")\n\n\n\n\n\n\n\n\nMid-way I also wanted to calculate the chances of staying below 21 while drawing cards using similar functions with some altercations:\n\nbetween0and21 &lt;- function(num_cards) {\n  deck &lt;- newdeck()\n  hand &lt;- sample(deck, num_cards, replace = FALSE)\n  hand_value &lt;- calculate_value(hand)\n  return(hand_value &gt;= 0 && hand_value &lt;= 21)\n}\n\n\ndraw_cards_alt &lt;- function(nums,reps){\n  probability &lt;- map_lgl(1:reps, ~between0and21(nums)) |&gt; mean()\n  return(data.frame(high_hand_prob = probability, amount_of_cards = nums))\n}\n\nIn this graph we see a representation of probabilities of staying under or at 21 when drawing cards. Similarly, we see another downward trend and a cut off at 6 cards since again it is impossible to be within 21 at such amount of cards. The line has a very straight slope and suggests that probability gets worse the more cards we get which makes sense as the deck of cards has more high value cards than lower values.\n\nset.seed(47)\nmap(2:10, ~draw_cards_alt(.x, reps = 10000)) |&gt; \n  list_rbind()|&gt;\n  ggplot(aes(x= amount_of_cards, y = high_hand_prob)) +\n  geom_line() +\n  labs(title = \"Probability of not going above 21 in Blackjack\" ,y = \"probability of having a hand value of 0-21\", x = \"number of cards drawn\")\n\n\n\n\n\n\n\n\n\n\nConclusion\nOverall my main goal was to calculate the chances of a player getting a good hand at the start of a blackjack game and wanted to continue seeing the probability of staying in that range throughout the game. I did this by having a simulation done 10000 times for if a player draws 2 cards all the way to 10 cards and see the probabilities between these choices. Throughout the code I explain what each one does. We can see such probabilities in the first plot and can conclude we have a high chance of getting a good hand in the first two cards and from there the probability gets worse as you play the game. This isn’t necessarily saying you have only a 30 percent chance to have a good hand in the game but it suggests that you can anticipate to win based on the lower amount of cards you have. This is also connected to the second graph calculating probabilities of staying under 21 based on the amount of cards you have. Both graphs have a similar trend and suggest the conclusion that you want to stay within a 3-4 card range when playing the game. To conclude, when playing blackjack you have the highest chance of winning (having a good hand) when starting the game and the more you hit (draw a card) the less chance you have at winning. Blackjack is a very probabilistic game as in this simulation I don’t take into account the decision of hitting or staying based on the dealers hand, since if the dealer has a bad hand, aka a hand that most likely will break 21, you don’t want to worsen your chances since you want to rely on them breaking first. I would love to continue running a simulation like this however it seems very complex and could get confusing but I am happy I got these results in the end.\nSome further codes I could develop, based on ideas suggested by classmates, is 1) implementing a dealers hand to accurately see if the user wins against whatever the dealer has as normally a 17-21 doens’t always constitute a win and 2) add in different playstyles for users such as a risky player, a counting player (who takes into the account of probability whether to draw or not) or a safe player. Overall, blackjack is a complicated game that can’t be taken completely as probability through simulations but we can always try to emulate it."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Leonardo Torres",
    "section": "",
    "text": "My name is Leonardo Torres, a junior at Pomona College majoring in Computer Science and Minoring in Math + Data Science. I am involved at the Draper Center where I coordinate two programs, Pomona Partners and Pomona College For Youth Success (PAYS).I enjoy being involved with affinity groups on campus and love spending time engaging with the community. I am also an officer with the 5C Club LatinX in Tech (LIT).\nThis past summer I spent the time doing research with Professor Eleanor Birrell under SURP. The paper we worked on is titled: Evaluating a Data Fiduciary Standard for Privacy: Developer and End-user Perspectives."
  },
  {
    "objectID": "DataVisualization.html",
    "href": "DataVisualization.html",
    "title": "Data Analysis",
    "section": "",
    "text": "My first step here is to load in the data files from github so it can be easily accessed within my quarto doc.\n\nrolling_stone &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2024/2024-05-07/rolling_stone.csv')"
  },
  {
    "objectID": "DataVisualization.html#installing-data",
    "href": "DataVisualization.html#installing-data",
    "title": "Data Analysis",
    "section": "",
    "text": "My first step here is to load in the data files from github so it can be easily accessed within my quarto doc.\n\nrolling_stone &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2024/2024-05-07/rolling_stone.csv')"
  },
  {
    "objectID": "DataVisualization.html#visualization",
    "href": "DataVisualization.html#visualization",
    "title": "Data Analysis",
    "section": "Visualization",
    "text": "Visualization\nUsing a simple ggplot function, we are able to use variables from the dataset to create a graph visualizing albums categorized by genres being released from 1950s to the present. We can see trends in this graph showing how hip-hop/rap grew in popularity after the early 1980s while Rock n ’Roll/Rhythm & Blues started to become less popular in the 1990s.\n\nlibrary(tidyverse)\nggplot(data = rolling_stone) +\n  geom_point(aes(x = release_year, y = genre, color = release_year))+\n  labs(\n    title = \"Genres of Albums throughout the Years\",\n    x = \"Release Years\",\n    y = \"Genres\"\n  )\n\n\n\n\n\n\n\n\nFrom this analysis, we can see a very clear difference throughout the decades of what music is popular or not. It is interesting to what music has died off such as jazz, while we see a major/sudden rise of others. This just shows how spontaneous music can be throughout the years with the rise and fall of trends. It just makes us realize that genres, artists, and even songs can go from 100 to 0 or 0 to 100."
  },
  {
    "objectID": "DataVisualization.html#reference",
    "href": "DataVisualization.html#reference",
    "title": "Data Analysis",
    "section": "Reference",
    "text": "Reference\nThe following dataset can be found at: https://github.com/rfordatascience/tidytuesday/blob/main/data/2024/2024-05-07/readme.md"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Resume as of 12/9/2024",
    "section": "",
    "text": "Resume Download\n\n\n\nResume"
  },
  {
    "objectID": "DataViz2.html",
    "href": "DataViz2.html",
    "title": "Data Analysis 2",
    "section": "",
    "text": "Similarly load in another dataset from github which can be easily accessed from our ufo_sightings variable.\n\nufo_sightings &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2023/2023-06-20/ufo_sightings.csv')\nplaces &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2023/2023-06-20/places.csv')\nday_parts_map &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2023/2023-06-20/day_parts_map.csv')"
  },
  {
    "objectID": "DataViz2.html#install-data",
    "href": "DataViz2.html#install-data",
    "title": "Data Analysis 2",
    "section": "",
    "text": "Similarly load in another dataset from github which can be easily accessed from our ufo_sightings variable.\n\nufo_sightings &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2023/2023-06-20/ufo_sightings.csv')\nplaces &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2023/2023-06-20/places.csv')\nday_parts_map &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2023/2023-06-20/day_parts_map.csv')"
  },
  {
    "objectID": "DataViz2.html#visualization",
    "href": "DataViz2.html#visualization",
    "title": "Data Analysis 2",
    "section": "Visualization",
    "text": "Visualization\nWe will create a vertical point bar graph to represent ufo sightings, categorized throughout states, in comparison to the year they were spotted.\n\nlibrary(tidyverse)\nggplot(data = filter(ufo_sightings, state %in% c(\"CA\", \"FL\", \"MA\", \"NV\", \"NM\", \"NY\", \"OH\", \"TX\", \"WA\", \"WY\"))) +\n  geom_point(aes(x = state, y = reported_date_time, color = reported_date_time, alpha= .5))+\n  labs(\n    x = \"State\",\n    y = \"Time\"\n  )\n\n\n\n\n\n\n\n\nFrom this we can see how in the states CA and NM the use had its first spotting in the late 1920s."
  },
  {
    "objectID": "DataViz2.html#references",
    "href": "DataViz2.html#references",
    "title": "Data Analysis 2",
    "section": "References",
    "text": "References\nhttps://github.com/rfordatascience/tidytuesday/blob/main/data/2023/2023-06-20/readme.md"
  },
  {
    "objectID": "presentation.html#purpose-and-goals",
    "href": "presentation.html#purpose-and-goals",
    "title": "Calculating Probability of Winning Blackjack using Simulations",
    "section": "Purpose and Goals",
    "text": "Purpose and Goals\nInterested in the probability of getting a good hand in blackjack, values between 17 to 21, through simulations.\n\nCreate a deck\nRandomly Chose n cards from deck\nCheck total value\nRun a Simulation (iterate through multiple times)"
  },
  {
    "objectID": "presentation.html#creating-a-deck",
    "href": "presentation.html#creating-a-deck",
    "title": "Calculating Probability of Winning Blackjack using Simulations",
    "section": "Creating a Deck",
    "text": "Creating a Deck\nUsing a function called newdeck, we have it represent a list/array of values which correlate with values in a normal card deck. Such as there being four 2 value cards, four 3 value cards, all the way to twelve 10 value cards representing the face cards (jack, queen, king)\n\nlibrary(tidyverse)\n\nnewdeck &lt;- function() {\n  deck &lt;- c(rep(2:10, each = 4), rep(10, 4*3), rep(11, 4))  \n  return(deck)\n}\n\n\ndeck &lt;- newdeck()\ndeck\n\n [1]  2  2  2  2  3  3  3  3  4  4  4  4  5  5  5  5  6  6  6  6  7  7  7  7  8\n[26]  8  8  8  9  9  9  9 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 11 11\n[51] 11 11"
  },
  {
    "objectID": "presentation.html#calculating-hand-value",
    "href": "presentation.html#calculating-hand-value",
    "title": "Calculating Probability of Winning Blackjack using Simulations",
    "section": "Calculating Hand Value",
    "text": "Calculating Hand Value\nFunction calculate_value is used to calculate the total amount of our hand given the numbers from the deck. It would be as simple as adding the hand however we have to take into consideration the ace values changing from 11 to 1 in the case our hand is above 21:\n\ncalculate_value &lt;- function(n) {\n  total &lt;- sum(n)\n  num_aces &lt;- sum(n == 11)\n  while(total &gt; 21 && num_aces &gt;0){\n    total &lt;- total - 10\n    num_aces &lt;- num_aces -1\n  }\n  return(total)\n}"
  },
  {
    "objectID": "presentation.html#check-if-hand-is-a-high-value.",
    "href": "presentation.html#check-if-hand-is-a-high-value.",
    "title": "Calculating Probability of Winning Blackjack using Simulations",
    "section": "Check if hand is a high value.",
    "text": "Check if hand is a high value.\n\nCreate deck\nDraw n cards from deck\nCalculate value\nReturn Logical value\n\n\nbetween17and21 &lt;- function(num_cards) {\n  deck &lt;- newdeck()\n  hand &lt;- sample(deck, num_cards, replace = FALSE)\n  hand_value &lt;- calculate_value(hand)\n  return(hand_value &gt;= 17 && hand_value &lt;= 21)\n}"
  },
  {
    "objectID": "presentation.html#run-simulation-with-2-cards-only",
    "href": "presentation.html#run-simulation-with-2-cards-only",
    "title": "Calculating Probability of Winning Blackjack using Simulations",
    "section": "Run Simulation with 2 cards only",
    "text": "Run Simulation with 2 cards only\nRuns the a simulation of drawing two cards 10000 times, calculating the mean probability of it being between 17 and 21 and returns a data frame of such statistics.\n\ndraw_cards &lt;- function(nums,reps){\n  probability &lt;- map_lgl(1:reps, ~between17and21(nums)) |&gt; mean()\n  return(data.frame(high_hand_prob = probability, amount_of_cards = nums))\n}\n\n\nset.seed(47)\ndraw_cards(2,10000)\n\n  high_hand_prob amount_of_cards\n1         0.3474               2"
  },
  {
    "objectID": "presentation.html#running-simulation-varying-2-to-10-cards",
    "href": "presentation.html#running-simulation-varying-2-to-10-cards",
    "title": "Calculating Probability of Winning Blackjack using Simulations",
    "section": "Running Simulation varying 2 to 10 cards",
    "text": "Running Simulation varying 2 to 10 cards\nUsing the mean probability of the multiple 10000 simulations ran, we plot the results on a line graph demonstrating the trend as we draw more cards. At around 5 cards is where we see a very low chance of having a high value hand, as most likely you will have gone above 21. At 2-3 cards its very unlikely to break 21 however it is still possible to be below 17.\n\nset.seed(47)\nmap(2:10, ~draw_cards(.x, reps = 10000)) |&gt; \n  list_rbind()|&gt;\n  ggplot(aes(x= amount_of_cards, y = high_hand_prob)) +\n  geom_line() +\n  labs(title = \"Probability of having a high hand value (17-21) in BlackJack\" ,y = \"probability of having a hand value of 17-21\", x = \"number of cards drawn\")"
  },
  {
    "objectID": "presentation.html#alternate-probability-calculation",
    "href": "presentation.html#alternate-probability-calculation",
    "title": "Calculating Probability of Winning Blackjack using Simulations",
    "section": "Alternate Probability Calculation",
    "text": "Alternate Probability Calculation\nInstead of getting the probability of having a high value hand, lets calculate the probability of having a hand below 21. Using similar functions but with changed logical values.\n\nbetween0and21 &lt;- function(num_cards) {\n  deck &lt;- newdeck()\n  hand &lt;- sample(deck, num_cards, replace = FALSE)\n  hand_value &lt;- calculate_value(hand)\n  return(hand_value &gt;= 0 && hand_value &lt;= 21)\n}\n\n\ndraw_cards_alt &lt;- function(nums,reps){\n  probability &lt;- map_lgl(1:reps, ~between0and21(nums)) |&gt; mean()\n  return(data.frame(high_hand_prob = probability, amount_of_cards = nums))\n}"
  },
  {
    "objectID": "presentation.html#probability-of-having-less-than-a-21-value-hand",
    "href": "presentation.html#probability-of-having-less-than-a-21-value-hand",
    "title": "Calculating Probability of Winning Blackjack using Simulations",
    "section": "Probability of having less than a 21 value hand",
    "text": "Probability of having less than a 21 value hand\nSimiarly to the previous graph, we can conclude that you would most likley break 21 after drawing 5 cards. It looks like the safe point is at 3 cards as at four your chances are cut more than half. This is due to the high quanity of face cards in the deck (value 10).\n\nset.seed(47)\nmap(2:10, ~draw_cards_alt(.x, reps = 10000)) |&gt; \n  list_rbind()|&gt;\n  ggplot(aes(x= amount_of_cards, y = high_hand_prob)) +\n  geom_line() +\n  labs(title = \"Probability of not going above 21 in Blackjack\" ,y = \"probability of having a hand value of 0-21\", x = \"number of cards drawn\")"
  },
  {
    "objectID": "presentation.html#conclusion-takeaways",
    "href": "presentation.html#conclusion-takeaways",
    "title": "Calculating Probability of Winning Blackjack using Simulations",
    "section": "Conclusion/ Takeaways",
    "text": "Conclusion/ Takeaways\n\nBlackjack has some luck involved(people consider it to be heavily skill based)\nOnly draw up to three cards\nDealer also has a hand, implement in the future.\nOverall, hard to simulate since there is a lot of factors when it comes to playing in real life"
  },
  {
    "objectID": "project4.html",
    "href": "project4.html",
    "title": "WAI Database Analysis + Freq. vs Mean Absorption",
    "section": "",
    "text": "In order to complete the task of copying figure one Voss (2020). my approach will be to look at the tables given from such database. From these databases,I will look at three tables specifically labeled as Subjects, PI_Info, and Measurements. I will have to join these tables together so that the information found within them are correlated with the correct publications. Specificaly from what it looks like the legend will be created from the subjects table while the measurement and PI_info tables will need to be joined accordingly."
  },
  {
    "objectID": "project4.html#starting-off",
    "href": "project4.html#starting-off",
    "title": "WAI Database Analysis + Freq. vs Mean Absorption",
    "section": "",
    "text": "In order to complete the task of copying figure one Voss (2020). my approach will be to look at the tables given from such database. From these databases,I will look at three tables specifically labeled as Subjects, PI_Info, and Measurements. I will have to join these tables together so that the information found within them are correlated with the correct publications. Specificaly from what it looks like the legend will be created from the subjects table while the measurement and PI_info tables will need to be joined accordingly."
  },
  {
    "objectID": "project4.html#sql-coding",
    "href": "project4.html#sql-coding",
    "title": "WAI Database Analysis + Freq. vs Mean Absorption",
    "section": "SQL Coding",
    "text": "SQL Coding\n\nlibrary(DBI)\nlibrary(tidyverse)\nlibrary(dbplyr)\nlibrary(dplyr)\nlibrary(RMariaDB)\n\n\ncon_wai &lt;- DBI::dbConnect(\n  RMariaDB::MariaDB(), host= \"scidb.smith.edu\",\n  user = \"waiuser\", password = \"smith_waiDB\", \n  dbname = \"wai\"\n)\nMeasurements &lt;- tbl(con_wai, \"Measurements\")\nPI_Info &lt;- tbl(con_wai, \"PI_Info\")\nSubjects &lt;- tbl(con_wai, \"Subjects\")\n\n# collect(Measurements)\n\n\nSHOW TABLES;\n\n\n7 records\n\n\nTables_in_wai\n\n\n\n\nCodebook\n\n\nMeasurements\n\n\nMeasurements_pre2020\n\n\nPI_Info\n\n\nPI_Info_OLD\n\n\nSubjects\n\n\nSubjects_pre2020\n\n\n\n\n\n\nDESCRIBE PI_Info;\n\n\nDisplaying records 1 - 10\n\n\nField\nType\nNull\nKey\nDefault\nExtra\n\n\n\n\nIdentifier\nvarchar(50)\nNO\nPRI\nNA\n\n\n\nYear\nint\nNO\n\nNA\n\n\n\nAuthors\ntext\nNO\n\nNA\n\n\n\nAuthorsShortList\ntext\nNO\n\nNA\n\n\n\nTitle\ntext\nNO\n\nNA\n\n\n\nJournal\ntext\nNO\n\nNA\n\n\n\nURL\ntext\nNO\n\nNA\n\n\n\nAbstract\ntext\nNO\n\nNA\n\n\n\nDataSubmitterName\ntext\nNO\n\nNA\n\n\n\nDataSubmitterEmail\ntext\nNO\n\nNA\n\n\n\n\n\n\n\nDESCRIBE Subjects;\n\n\nDisplaying records 1 - 10\n\n\n\n\n\n\n\n\n\n\nField\nType\nNull\nKey\nDefault\nExtra\n\n\n\n\nIdentifier\nvarchar(50)\nNO\nPRI\nNA\n\n\n\nSubjectNumber\nint\nNO\nPRI\nNA\n\n\n\nSessionTotal\nint\nNO\n\nNA\n\n\n\nAgeFirstMeasurement\nfloat\nYES\n\nNA\n\n\n\nAgeCategoryFirstMeasurement\nvarchar(50)\nYES\n\nNA\n\n\n\nSex\nvarchar(50)\nNO\n\nNA\n\n\n\nRace\nvarchar(50)\nNO\n\nNA\n\n\n\nEthnicity\nvarchar(50)\nNO\n\nNA\n\n\n\nLeftEarStatusFirstMeasurement\nvarchar(50)\nNO\n\nNA\n\n\n\nRightEarStatusFirstMeasurement\nvarchar(50)\nNO\n\nNA\n\n\n\n\n\n\n\nSELECT *\nFROM Measurements\nLIMIT 0,50;\n\n\nDisplaying records 1 - 10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIdentifier\nSubjectNumber\nSession\nEar\nInstrument\nAge\nAgeCategory\nEarStatus\nTPP\nAreaCanal\nPressureCanal\nSweepDirection\nFrequency\nAbsorbance\nZmag\nZang\n\n\n\n\nAbur_2014\n1\n1\nLeft\nHearID\n20\nAdult\nNormal\n-5\n4.42e-05\n0\nAmbient\n210.938\n0.0333379\n113780000\n-0.233504\n\n\nAbur_2014\n1\n1\nLeft\nHearID\n20\nAdult\nNormal\n-5\n4.42e-05\n0\nAmbient\n234.375\n0.0315705\n103585000\n-0.235778\n\n\nAbur_2014\n1\n1\nLeft\nHearID\n20\nAdult\nNormal\n-5\n4.42e-05\n0\nAmbient\n257.812\n0.0405751\n92951696\n-0.233482\n\n\nAbur_2014\n1\n1\nLeft\nHearID\n20\nAdult\nNormal\n-5\n4.42e-05\n0\nAmbient\n281.250\n0.0438399\n86058000\n-0.233421\n\n\nAbur_2014\n1\n1\nLeft\nHearID\n20\nAdult\nNormal\n-5\n4.42e-05\n0\nAmbient\n304.688\n0.0486400\n79492800\n-0.232931\n\n\nAbur_2014\n1\n1\nLeft\nHearID\n20\nAdult\nNormal\n-5\n4.42e-05\n0\nAmbient\n328.125\n0.0527801\n73326200\n-0.232837\n\n\nAbur_2014\n1\n1\nLeft\nHearID\n20\nAdult\nNormal\n-5\n4.42e-05\n0\nAmbient\n351.562\n0.0583192\n68793600\n-0.232115\n\n\nAbur_2014\n1\n1\nLeft\nHearID\n20\nAdult\nNormal\n-5\n4.42e-05\n0\nAmbient\n375.000\n0.0638881\n64088600\n-0.231642\n\n\nAbur_2014\n1\n1\nLeft\nHearID\n20\nAdult\nNormal\n-5\n4.42e-05\n0\nAmbient\n398.438\n0.0687025\n60200600\n-0.231356\n\n\nAbur_2014\n1\n1\nLeft\nHearID\n20\nAdult\nNormal\n-5\n4.42e-05\n0\nAmbient\n421.875\n0.0833181\n56990900\n-0.228356"
  },
  {
    "objectID": "project4.html#table-for-freq-and-absorbance",
    "href": "project4.html#table-for-freq-and-absorbance",
    "title": "WAI Database Analysis + Freq. vs Mean Absorption",
    "section": "Table for Freq and Absorbance",
    "text": "Table for Freq and Absorbance\n\nSELECT \n  PI_Info.Identifier, \n  PI_Info.AuthorsShortList, \n  Frequency, \n  AVG(Absorbance) AS absorbance, \n  PI_Info.Year\nFROM PI_Info\nLEFT JOIN Measurements \nON PI_Info.Identifier = Measurements.Identifier\nWHERE Measurements.Identifier IN (\n  \"Abur_2014\", \"Feeney_2017\", \"Groon_2015\", \"Lewis_2015\", \"Liu_2008\",\n  \"Rosowski_2012\", \"Shahnaz_2006\", \"Shaver_2013\", \"Sun_2016\", \"Voss_1994\"\n  \"Voss_2010\", \"Werner_2010\") \n AND Frequency BETWEEN 200 AND 8000\nGROUP BY Identifier, Frequency;"
  },
  {
    "objectID": "project4.html#table-for-legend",
    "href": "project4.html#table-for-legend",
    "title": "WAI Database Analysis + Freq. vs Mean Absorption",
    "section": "Table for Legend",
    "text": "Table for Legend\n\nSELECT \n  AuthorsShortList, \n  Year, \n  COUNT(DISTINCT CONCAT(SubjectNumber, '_' ,Ear)) AS N,\n  Instrument, Frequency, AVG(Absorbance) AS absorbance,\n  CONCAT(\n  AuthorsShortList, \" (\",Year,\") \",\n  \"N=\", COUNT(DISTINCT SubjectNumber, Ear),\n  \"; \", Instrument) AS legend_graph\nFROM PI_Info AS p\nLEFT JOIN Measurements AS m \n  ON m.Identifier = p.Identifier \nWHERE p.Identifier IN (\n  \"Abur_2014\", \"Feeney_2017\", \"Groon_2015\", \"Lewis_2015\", \"Liu_2008\",\n  \"Rosowski_2012\", \"Shahnaz_2006\", \"Shaver_2013\", \"Sun_2016\", \"Voss_1994\",\n  \"Voss_2010\", \"Werner_2010\") \n  AND Frequency BETWEEN 200 AND 8000\nGROUP BY Frequency, p.Identifier, m.Instrument"
  },
  {
    "objectID": "project4.html#graph-replicate",
    "href": "project4.html#graph-replicate",
    "title": "WAI Database Analysis + Freq. vs Mean Absorption",
    "section": "Graph Replicate",
    "text": "Graph Replicate\n\ninfo |&gt; ggplot(aes(x = Frequency, y = absorbance, group = legend_graph ,color = legend_graph)) +\n  labs(\n    title = \"Mean absorbance from each publication in WAI database\",\n    x = \"Frequency (Hz)\", \n    y = \"Mean Absorbance\",\n  ) +\n  geom_line() +\n  scale_x_log10() +\n  theme_minimal() +\n  theme(\n    legend.position = c(0.2,0.78),\n    legend.key.height = unit(0.2, \"cm\"),\n  )"
  },
  {
    "objectID": "project4.html#end-description",
    "href": "project4.html#end-description",
    "title": "WAI Database Analysis + Freq. vs Mean Absorption",
    "section": "End Description",
    "text": "End Description\nAlthough the graph above doesn’t look one to one to figure one, it appears to be very similar regarding its trend and location within this plot. As discussed in the beginning, the main goal was to join PI_Info and Measurements together through the Identifier column, since this is what identifies which row is a specific publication. From this a table was created that describes the freq. and mean absorbances for each publication. The next chunk of SQL code joins measurements and PI_Info again however the main purpose of it was to get the the information to create the legend on the given graph. Finally with both tables, I was able to create a plot graph that shows the mean absorbance from each publication depending on its frequencies."
  },
  {
    "objectID": "project4.html#focus-on-wai-study",
    "href": "project4.html#focus-on-wai-study",
    "title": "WAI Database Analysis + Freq. vs Mean Absorption",
    "section": "Focus on WAI Study",
    "text": "Focus on WAI Study\nAs a study I wanted to look into, I focused on Myers_2018 who only researched on babies/infants hearing. Because of this I can’t analyze or look at age groups so I decided that sex would be a good option for this.\n\nSELECT *\nFrom Subjects\nWHERE Identifier = \"Myers_2018\"\nLIMIT 0, 10\n\n\nDisplaying records 1 - 10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIdentifier\nSubjectNumber\nSessionTotal\nAgeFirstMeasurement\nAgeCategoryFirstMeasurement\nSex\nRace\nEthnicity\nLeftEarStatusFirstMeasurement\nRightEarStatusFirstMeasurement\nSubjectNotes\n\n\n\n\nMyers_2018\n7\n1\n0.0062785\nInfant\nMale\nUnknown\nUnknown\nUnknown\nUnknown\nR= unknown L= unknown\n\n\nMyers_2018\n8\n1\n0.0035388\nInfant\nMale\nUnknown\nUnknown\nAbnormal\nAbnormal\nR= tymp pass only L= tymp pass only\n\n\nMyers_2018\n9\n1\n0.0043379\nInfant\nMale\nUnknown\nUnknown\nNormal\nNormal\nR= pass L= pass\n\n\nMyers_2018\n12\n1\n0.0086758\nInfant\nMale\nUnknown\nUnknown\nUnknown\nNormal\nR= pass L= unknown\n\n\nMyers_2018\n13\n1\n0.0085616\nInfant\nFemale\nUnknown\nUnknown\nNormal\nNormal\nR= pass L= pass\n\n\nMyers_2018\n14\n1\n0.0094749\nInfant\nFemale\nUnknown\nUnknown\nNormal\nNormal\nR= pass L= pass\n\n\nMyers_2018\n16\n1\n0.0046804\nInfant\nFemale\nUnknown\nUnknown\nNormal\nNormal\nR= pass L= pass\n\n\nMyers_2018\n17\n1\n0.0052511\nInfant\nMale\nUnknown\nUnknown\nNormal\nNormal\nR= pass L= pass\n\n\nMyers_2018\n18\n1\n0.0028539\nInfant\nMale\nUnknown\nUnknown\nNormal\nNormal\nR= pass L= pass\n\n\nMyers_2018\n19\n1\n0.0026256\nInfant\nFemale\nUnknown\nUnknown\nNormal\nNormal\nR= pass L= pass\n\n\n\n\n\nSimilar code to what was done previously.\n\nSELECT Measurements.Identifier,\n        AVG(Absorbance) AS Mean_Absorbance,\n        Frequency,\n        CONCAT(AuthorsShortList, \" (\", Year, \") \", \" N= \", \n              COUNT(DISTINCT Measurements.SubjectNumber, Ear), \n              \" ; \", Instrument) AS Label, Sex\nFROM Measurements\nJOIN PI_Info ON Measurements.Identifier = PI_Info.Identifier \nJOIN Subjects ON Subjects.SubjectNumber = Measurements.SubjectNumber\nWHERE Measurements.Identifier = \"Myers_2018\" AND\n      Frequency BETWEEN 200 AND 8000\nGROUP BY Sex, Frequency, Instrument;\n\n\nhead(Myers_2018)\n\n  Identifier Mean_Absorbance Frequency                             Label    Sex\n1 Myers_2018          0.4000    226.00 Myers et al. (2018)  N= 2 ; Titan Femake\n2 Myers_2018          0.3920    257.33 Myers et al. (2018)  N= 2 ; Titan Femake\n3 Myers_2018          0.3915    280.62 Myers et al. (2018)  N= 2 ; Titan Femake\n4 Myers_2018          0.3925    297.30 Myers et al. (2018)  N= 2 ; Titan Femake\n5 Myers_2018          0.3910    324.21 Myers et al. (2018)  N= 2 ; Titan Femake\n6 Myers_2018          0.3905    343.49 Myers et al. (2018)  N= 2 ; Titan Femake"
  },
  {
    "objectID": "project4.html#graph",
    "href": "project4.html#graph",
    "title": "WAI Database Analysis + Freq. vs Mean Absorption",
    "section": "Graph",
    "text": "Graph\n\nMyers_2018 |&gt;\n  ggplot(aes(x = Frequency, y = Mean_Absorbance, color = Sex)) +\n  geom_line() +\n  labs(\n    title = \"Mean absorbance grouped by Sex. Myers 2018 study\",\n    x = \"Frequency(Hz)\",\n    y = \"Mean Absorbance\",\n  ) +\n  scale_x_log10() +\n  theme_minimal() +\n  theme(\n    legend.position = c(0.2,0.78),\n    legend.key.height = unit(0.2, \"cm\"))"
  },
  {
    "objectID": "project4.html#conclusion-for-study",
    "href": "project4.html#conclusion-for-study",
    "title": "WAI Database Analysis + Freq. vs Mean Absorption",
    "section": "Conclusion for Study",
    "text": "Conclusion for Study\nAfter gathering all the data from the Myers 2018 study, I am a bit confused on the final outcome of said study because I ended up with Femake as a sex and two Males. It might have been an error when I was mutating however I reset everything and still was left with such results.There is a large difference in means for “Femakes” while the other sexs re about the same similarlity.\n\nDBI::dbDisconnect(con_wai, shutdown = TRUE)"
  },
  {
    "objectID": "project4.html#references",
    "href": "project4.html#references",
    "title": "WAI Database Analysis + Freq. vs Mean Absorption",
    "section": "References",
    "text": "References\nVoss, SE. 2019. “Resource Review.” Ear and Hearing 40 (6). https://doi.org/10.1097/AUD.0000000000000790.\nWAI DataBase: Citation: doi.org/10.35482/egr.001.2022"
  }
]
[
  {
    "objectID": "project4.html",
    "href": "project4.html",
    "title": "Project 4: SQL Query",
    "section": "",
    "text": "In order to complete the task of copying figure one Voss (2020). my approach will be to look at the tables given from such database. From these databases,I will look at three tables specifically labeled as Subjects, PI_Info, and Measurements. I will have to join these tables together so that the information found within them are correlated with the correct publications. Specificaly from what it looks like the legend will be created from the subjects table while the measurement and PI_info tables will need to be joined accordingly.\n##SQL Coding\n\nlibrary(DBI)\nlibrary(tidyverse)\nlibrary(dbplyr)\nlibrary(dplyr)\nlibrary(RMariaDB)\n\n\ncon_wai &lt;- DBI::dbConnect(\n  RMariaDB::MariaDB(), host= \"scidb.smith.edu\",\n  user = \"waiuser\", password = \"smith_waiDB\", \n  dbname = \"wai\"\n)\nMeasurements &lt;- tbl(con_wai, \"Measurements\")\nPI_Info &lt;- tbl(con_wai, \"PI_Info\")\nSubjects &lt;- tbl(con_wai, \"Subjects\")\n\n# collect(Measurements)\n\n\nSHOW TABLES;\n\n\n7 records\n\n\nTables_in_wai\n\n\n\n\nCodebook\n\n\nMeasurements\n\n\nMeasurements_pre2020\n\n\nPI_Info\n\n\nPI_Info_OLD\n\n\nSubjects\n\n\nSubjects_pre2020\n\n\n\n\n\n\nDESCRIBE PI_Info;\n\n\nDisplaying records 1 - 10\n\n\nField\nType\nNull\nKey\nDefault\nExtra\n\n\n\n\nIdentifier\nvarchar(50)\nNO\nPRI\nNA\n\n\n\nYear\nint\nNO\n\nNA\n\n\n\nAuthors\ntext\nNO\n\nNA\n\n\n\nAuthorsShortList\ntext\nNO\n\nNA\n\n\n\nTitle\ntext\nNO\n\nNA\n\n\n\nJournal\ntext\nNO\n\nNA\n\n\n\nURL\ntext\nNO\n\nNA\n\n\n\nAbstract\ntext\nNO\n\nNA\n\n\n\nDataSubmitterName\ntext\nNO\n\nNA\n\n\n\nDataSubmitterEmail\ntext\nNO\n\nNA\n\n\n\n\n\n\n\nDESCRIBE Subjects;\n\n\nDisplaying records 1 - 10\n\n\n\n\n\n\n\n\n\n\nField\nType\nNull\nKey\nDefault\nExtra\n\n\n\n\nIdentifier\nvarchar(50)\nNO\nPRI\nNA\n\n\n\nSubjectNumber\nint\nNO\nPRI\nNA\n\n\n\nSessionTotal\nint\nNO\n\nNA\n\n\n\nAgeFirstMeasurement\nfloat\nYES\n\nNA\n\n\n\nAgeCategoryFirstMeasurement\nvarchar(50)\nYES\n\nNA\n\n\n\nSex\nvarchar(50)\nNO\n\nNA\n\n\n\nRace\nvarchar(50)\nNO\n\nNA\n\n\n\nEthnicity\nvarchar(50)\nNO\n\nNA\n\n\n\nLeftEarStatusFirstMeasurement\nvarchar(50)\nNO\n\nNA\n\n\n\nRightEarStatusFirstMeasurement\nvarchar(50)\nNO\n\nNA\n\n\n\n\n\n\n\nSELECT *\nFROM Measurements\nLIMIT 0,50;\n\n\nDisplaying records 1 - 10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIdentifier\nSubjectNumber\nSession\nEar\nInstrument\nAge\nAgeCategory\nEarStatus\nTPP\nAreaCanal\nPressureCanal\nSweepDirection\nFrequency\nAbsorbance\nZmag\nZang\n\n\n\n\nAbur_2014\n1\n1\nLeft\nHearID\n20\nAdult\nNormal\n-5\n4.42e-05\n0\nAmbient\n210.938\n0.0333379\n113780000\n-0.233504\n\n\nAbur_2014\n1\n1\nLeft\nHearID\n20\nAdult\nNormal\n-5\n4.42e-05\n0\nAmbient\n234.375\n0.0315705\n103585000\n-0.235778\n\n\nAbur_2014\n1\n1\nLeft\nHearID\n20\nAdult\nNormal\n-5\n4.42e-05\n0\nAmbient\n257.812\n0.0405751\n92951696\n-0.233482\n\n\nAbur_2014\n1\n1\nLeft\nHearID\n20\nAdult\nNormal\n-5\n4.42e-05\n0\nAmbient\n281.250\n0.0438399\n86058000\n-0.233421\n\n\nAbur_2014\n1\n1\nLeft\nHearID\n20\nAdult\nNormal\n-5\n4.42e-05\n0\nAmbient\n304.688\n0.0486400\n79492800\n-0.232931\n\n\nAbur_2014\n1\n1\nLeft\nHearID\n20\nAdult\nNormal\n-5\n4.42e-05\n0\nAmbient\n328.125\n0.0527801\n73326200\n-0.232837\n\n\nAbur_2014\n1\n1\nLeft\nHearID\n20\nAdult\nNormal\n-5\n4.42e-05\n0\nAmbient\n351.562\n0.0583192\n68793600\n-0.232115\n\n\nAbur_2014\n1\n1\nLeft\nHearID\n20\nAdult\nNormal\n-5\n4.42e-05\n0\nAmbient\n375.000\n0.0638881\n64088600\n-0.231642\n\n\nAbur_2014\n1\n1\nLeft\nHearID\n20\nAdult\nNormal\n-5\n4.42e-05\n0\nAmbient\n398.438\n0.0687025\n60200600\n-0.231356\n\n\nAbur_2014\n1\n1\nLeft\nHearID\n20\nAdult\nNormal\n-5\n4.42e-05\n0\nAmbient\n421.875\n0.0833181\n56990900\n-0.228356"
  },
  {
    "objectID": "project4.html#starting-off",
    "href": "project4.html#starting-off",
    "title": "Project 4: SQL Query",
    "section": "",
    "text": "In order to complete the task of copying figure one Voss (2020). my approach will be to look at the tables given from such database. From these databases,I will look at three tables specifically labeled as Subjects, PI_Info, and Measurements. I will have to join these tables together so that the information found within them are correlated with the correct publications. Specificaly from what it looks like the legend will be created from the subjects table while the measurement and PI_info tables will need to be joined accordingly.\n##SQL Coding\n\nlibrary(DBI)\nlibrary(tidyverse)\nlibrary(dbplyr)\nlibrary(dplyr)\nlibrary(RMariaDB)\n\n\ncon_wai &lt;- DBI::dbConnect(\n  RMariaDB::MariaDB(), host= \"scidb.smith.edu\",\n  user = \"waiuser\", password = \"smith_waiDB\", \n  dbname = \"wai\"\n)\nMeasurements &lt;- tbl(con_wai, \"Measurements\")\nPI_Info &lt;- tbl(con_wai, \"PI_Info\")\nSubjects &lt;- tbl(con_wai, \"Subjects\")\n\n# collect(Measurements)\n\n\nSHOW TABLES;\n\n\n7 records\n\n\nTables_in_wai\n\n\n\n\nCodebook\n\n\nMeasurements\n\n\nMeasurements_pre2020\n\n\nPI_Info\n\n\nPI_Info_OLD\n\n\nSubjects\n\n\nSubjects_pre2020\n\n\n\n\n\n\nDESCRIBE PI_Info;\n\n\nDisplaying records 1 - 10\n\n\nField\nType\nNull\nKey\nDefault\nExtra\n\n\n\n\nIdentifier\nvarchar(50)\nNO\nPRI\nNA\n\n\n\nYear\nint\nNO\n\nNA\n\n\n\nAuthors\ntext\nNO\n\nNA\n\n\n\nAuthorsShortList\ntext\nNO\n\nNA\n\n\n\nTitle\ntext\nNO\n\nNA\n\n\n\nJournal\ntext\nNO\n\nNA\n\n\n\nURL\ntext\nNO\n\nNA\n\n\n\nAbstract\ntext\nNO\n\nNA\n\n\n\nDataSubmitterName\ntext\nNO\n\nNA\n\n\n\nDataSubmitterEmail\ntext\nNO\n\nNA\n\n\n\n\n\n\n\nDESCRIBE Subjects;\n\n\nDisplaying records 1 - 10\n\n\n\n\n\n\n\n\n\n\nField\nType\nNull\nKey\nDefault\nExtra\n\n\n\n\nIdentifier\nvarchar(50)\nNO\nPRI\nNA\n\n\n\nSubjectNumber\nint\nNO\nPRI\nNA\n\n\n\nSessionTotal\nint\nNO\n\nNA\n\n\n\nAgeFirstMeasurement\nfloat\nYES\n\nNA\n\n\n\nAgeCategoryFirstMeasurement\nvarchar(50)\nYES\n\nNA\n\n\n\nSex\nvarchar(50)\nNO\n\nNA\n\n\n\nRace\nvarchar(50)\nNO\n\nNA\n\n\n\nEthnicity\nvarchar(50)\nNO\n\nNA\n\n\n\nLeftEarStatusFirstMeasurement\nvarchar(50)\nNO\n\nNA\n\n\n\nRightEarStatusFirstMeasurement\nvarchar(50)\nNO\n\nNA\n\n\n\n\n\n\n\nSELECT *\nFROM Measurements\nLIMIT 0,50;\n\n\nDisplaying records 1 - 10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIdentifier\nSubjectNumber\nSession\nEar\nInstrument\nAge\nAgeCategory\nEarStatus\nTPP\nAreaCanal\nPressureCanal\nSweepDirection\nFrequency\nAbsorbance\nZmag\nZang\n\n\n\n\nAbur_2014\n1\n1\nLeft\nHearID\n20\nAdult\nNormal\n-5\n4.42e-05\n0\nAmbient\n210.938\n0.0333379\n113780000\n-0.233504\n\n\nAbur_2014\n1\n1\nLeft\nHearID\n20\nAdult\nNormal\n-5\n4.42e-05\n0\nAmbient\n234.375\n0.0315705\n103585000\n-0.235778\n\n\nAbur_2014\n1\n1\nLeft\nHearID\n20\nAdult\nNormal\n-5\n4.42e-05\n0\nAmbient\n257.812\n0.0405751\n92951696\n-0.233482\n\n\nAbur_2014\n1\n1\nLeft\nHearID\n20\nAdult\nNormal\n-5\n4.42e-05\n0\nAmbient\n281.250\n0.0438399\n86058000\n-0.233421\n\n\nAbur_2014\n1\n1\nLeft\nHearID\n20\nAdult\nNormal\n-5\n4.42e-05\n0\nAmbient\n304.688\n0.0486400\n79492800\n-0.232931\n\n\nAbur_2014\n1\n1\nLeft\nHearID\n20\nAdult\nNormal\n-5\n4.42e-05\n0\nAmbient\n328.125\n0.0527801\n73326200\n-0.232837\n\n\nAbur_2014\n1\n1\nLeft\nHearID\n20\nAdult\nNormal\n-5\n4.42e-05\n0\nAmbient\n351.562\n0.0583192\n68793600\n-0.232115\n\n\nAbur_2014\n1\n1\nLeft\nHearID\n20\nAdult\nNormal\n-5\n4.42e-05\n0\nAmbient\n375.000\n0.0638881\n64088600\n-0.231642\n\n\nAbur_2014\n1\n1\nLeft\nHearID\n20\nAdult\nNormal\n-5\n4.42e-05\n0\nAmbient\n398.438\n0.0687025\n60200600\n-0.231356\n\n\nAbur_2014\n1\n1\nLeft\nHearID\n20\nAdult\nNormal\n-5\n4.42e-05\n0\nAmbient\n421.875\n0.0833181\n56990900\n-0.228356"
  },
  {
    "objectID": "project4.html#table-for-freq-and-absorbance",
    "href": "project4.html#table-for-freq-and-absorbance",
    "title": "Project 4: SQL Query",
    "section": "Table for Freq and Absorbance",
    "text": "Table for Freq and Absorbance\n\nSELECT \n  PI_Info.Identifier, \n  PI_Info.AuthorsShortList, \n  Frequency, \n  AVG(Absorbance) AS absorbance, \n  PI_Info.Year\nFROM PI_Info\nLEFT JOIN Measurements \nON PI_Info.Identifier = Measurements.Identifier\nWHERE Measurements.Identifier IN (\n  \"Abur_2014\", \"Feeney_2017\", \"Groon_2015\", \"Lewis_2015\", \"Liu_2008\",\n  \"Rosowski_2012\", \"Shahnaz_2006\", \"Shaver_2013\", \"Sun_2016\", \"Voss_1994\"\n  \"Voss_2010\", \"Werner_2010\") \n AND Frequency BETWEEN 200 AND 8000\nGROUP BY Identifier, Frequency;"
  },
  {
    "objectID": "project4.html#table-for-legend",
    "href": "project4.html#table-for-legend",
    "title": "Project 4: SQL Query",
    "section": "Table for Legend",
    "text": "Table for Legend\n\nSELECT \n  AuthorsShortList, \n  Year, \n  COUNT(DISTINCT CONCAT(SubjectNumber, '_' ,Ear)) AS N,\n  Instrument, Frequency, AVG(Absorbance) AS absorbance,\n  CONCAT(\n  AuthorsShortList, \" (\",Year,\") \",\n  \"N=\", COUNT(DISTINCT SubjectNumber, Ear),\n  \"; \", Instrument) AS legend_graph\nFROM PI_Info AS p\nLEFT JOIN Measurements AS m \n  ON m.Identifier = p.Identifier \nWHERE p.Identifier IN (\n  \"Abur_2014\", \"Feeney_2017\", \"Groon_2015\", \"Lewis_2015\", \"Liu_2008\",\n  \"Rosowski_2012\", \"Shahnaz_2006\", \"Shaver_2013\", \"Sun_2016\", \"Voss_1994\",\n  \"Voss_2010\", \"Werner_2010\") \n  AND Frequency BETWEEN 200 AND 8000\nGROUP BY Frequency, p.Identifier, m.Instrument"
  },
  {
    "objectID": "project4.html#graph-replicate",
    "href": "project4.html#graph-replicate",
    "title": "Project 4: SQL Query",
    "section": "Graph Replicate",
    "text": "Graph Replicate\n\ninfo |&gt; ggplot(aes(x = Frequency, y = absorbance, group = legend_graph ,color = legend_graph)) +\n  labs(\n    title = \"Mean absorbance from each publication in WAI database\",\n    x = \"Frequency (Hz)\", \n    y = \"Mean Absorbance\",\n  ) +\n  geom_line() +\n  scale_x_log10() +\n  theme_minimal() +\n  theme(\n    legend.position = c(0.2,0.78),\n    legend.key.height = unit(0.2, \"cm\"),\n  )"
  },
  {
    "objectID": "project4.html#end-description",
    "href": "project4.html#end-description",
    "title": "Project 4: SQL Query",
    "section": "End Description",
    "text": "End Description\nAlthough the graph above doesn’t look one to one to figure one, it appears to be very similar regarding its trend and location within this plot. As discussed in the beginning, the main goal was to join PI_Info and Measurements together through the Identifier column, since this is what identifies which row is a specific publication. From this a table was created that describes the freq. and mean absorbances for each publication. The next chunk of SQL code joins measurements and PI_Info again however the main purpose of it was to get the the information to create the legend on the given graph. Finally with both tables, I was able to create a plot graph that shows the mean absorbance from each publication depending on its frequencies."
  },
  {
    "objectID": "project4.html#individual-study",
    "href": "project4.html#individual-study",
    "title": "Project 4: SQL Query",
    "section": "Individual Study",
    "text": "Individual Study\nAfter looking through all of tidyverse, I wasn’t able to find a good data set I liked that had sex, race, ethnicity, or age groups that would be best to use for a frequency vs mean absorbance graph. I decided to go on kaggle where I found this public database regarding covid data found at this link. https://www.kaggle.com/code/kerneler/starter-covid-19-cases-and-deaths-by-54180eb0-f/comments\nThis dataset contains a Race/Ethnicity column that sepeartes Hispanics, American Indian, Asian, Pacific islander, Black, and etc. I thought this would be perfect and decied to read the csv file I downloaded and go from there\n\ncovid_data &lt;- read_csv(\"COVID-19_Cases_and_Deaths_by_Race_Ethnicity.csv\", show_col_types = FALSE)\ncovid_data\n\n# A tibble: 382 × 9\n   `Date updated` `Race/ethnicity`              `Total population` `Total cases`\n   &lt;chr&gt;          &lt;chr&gt;                                      &lt;dbl&gt;         &lt;dbl&gt;\n 1 04/20/2020     Hispanic                                  589809          2269\n 2 04/20/2020     NH Asian                                  187532           215\n 3 04/20/2020     NH Black                                  387134          1823\n 4 04/20/2020     NH Other                                       0           118\n 5 04/20/2020     NH White                                 2408190          5101\n 6 04/20/2020     Unknown                                        0          8837\n 7 05/05/2020     Hispanic                                  589809          5079\n 8 05/05/2020     NH American Indian or Alaska…               9566            30\n 9 05/05/2020     NH Asian or Pacific Islander              177966           444\n10 05/05/2020     NH Black                                  387134          3720\n# ℹ 372 more rows\n# ℹ 5 more variables: `Crude case rate per 100k` &lt;dbl&gt;,\n#   `Age adjusted case rate per 100k` &lt;dbl&gt;, `Total deaths` &lt;dbl&gt;,\n#   `Crude death rate per 100k` &lt;dbl&gt;, `Age adjusted death rate per 100k` &lt;dbl&gt;\n\n\nThe main issue I encountered during this was trying to use SQL on this data set. I wasn’t able to set a connection or use SQL commands since I loaded it in R using the read_csv command so it was very difficult to set up a frequency vs mean absorbance graph found in the beginning of this project. I figured I would try to create this graph in a similar format through the use of R instead.\nBelow you will find code of me gathering summarizes from the covid data set and graphing two different graphs, a bar graph and an attempt at a line graph.\nHopefully with more time I can fix this and be able to use SQL to do joining on this data set, possibly I could focus on this for a portion of project 5.\n\nlibrary(readr)\nlibrary(dplyr)\nlibrary(ggplot2)\n\n# Summarize data: calculate frequency, mean cases, and mean deaths by race/ethnicity\nsummary_covid_cases &lt;- covid_data |&gt;\n  group_by(`Race/ethnicity`) |&gt;\n  summarize(\n    Frequency = n(),\n    MeanCases = mean(`Total cases`, na.rm = TRUE),\n  )\n\n# Plot frequency vs. mean cases by race/ethnicity\nggplot(summary_covid_cases, aes(x = Frequency, y = MeanCases, fill = `Race/ethnicity`)) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  theme_minimal() +\n  labs(\n    title = \"Frequency vs. Mean COVID-19 Cases by Race/Ethnicity\",\n    x = \"Frequency\",\n    y = \"Mean Cases\"\n  ) +\n  theme(\n    legend.title = element_blank(),\n    plot.title = element_text(hjust = 0.5)\n  )\n\n\n\n\n\n\n\n\n\nlibrary(readr)\nlibrary(dplyr)\nlibrary(ggplot2)\n\n# Summarize data: calculate frequency, mean cases by race/ethnicity\nsummary_covid_cases &lt;- covid_data |&gt;\n  group_by(`Race/ethnicity`) |&gt;\n  summarize(\n    Frequency = n(),\n    MeanCases = mean(`Total cases`, na.rm = TRUE)\n  ) |&gt;\n  arrange(Frequency)  # Order by Frequency to enable smooth line plotting\n\n# Plot frequency vs. mean cases as a line graph\nggplot(summary_covid_cases, aes(x = Frequency, y = MeanCases, color = `Race/ethnicity`)) +\n  geom_line(size = 1) + # Add lines connecting the data points\n  geom_point(size = 3) + # Highlight points for clarity\n  theme_minimal() +\n  labs(\n    title = \"Frequency vs. Mean COVID-19 Cases by Race/Ethnicity\",\n    x = \"Frequency\",\n    y = \"Mean Cases\"\n  ) +\n  theme(\n    legend.title = element_blank(),\n    plot.title = element_text(hjust = 0.5)\n  )"
  },
  {
    "objectID": "project4.html#conclusion",
    "href": "project4.html#conclusion",
    "title": "Project 4: SQL Query",
    "section": "Conclusion",
    "text": "Conclusion\nBoth graphs display similar results as it simply highlights the mean case for that race group. In both we can see the highest mean for cases would be for whites, followed by unknown, hispanics, and so on. My attempt at this failed and it has made me realize that SQL in some cases such as this are far more superior to use as SQL allows it easy to join groups associated which each other along with being able to group by. Although R has this feature as well. SQL makes it much more easy and straightforward due to its organized and hierarchy structure."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Leonardo Torres",
    "section": "",
    "text": "My name is Leonardo Torres, a junior at Pomona College majoring in Computer Science and Minoring in Math + Data Science. I am very involved at the Draper Center where I coordinator two programs, Pomona Partners and PAYS. I enjoy being involved with affinity groups on campus and love spending time engaging with the community. I am also an officer with the 5C Club LatinX in Tech (LIT)."
  },
  {
    "objectID": "DataVisualization.html",
    "href": "DataVisualization.html",
    "title": "Data Analysis",
    "section": "",
    "text": "# Option 2: Read directly from GitHub\n\nrolling_stone &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2024/2024-05-07/rolling_stone.csv')\n\n\nlibrary(tidyverse)\nggplot(data = rolling_stone) +\n  geom_point(aes(x = release_year, y = genre, color = release_year))+\n  labs(\n    x = \"Release Years\",\n    y = \"Genres\"\n  )\n\n\n\n\n\n\n\n\n\nufo_sightings &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2023/2023-06-20/ufo_sightings.csv')\nplaces &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2023/2023-06-20/places.csv')\nday_parts_map &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2023/2023-06-20/day_parts_map.csv')\n\n\nlibrary(tidyverse)\nggplot(data = filter(ufo_sightings, state %in% c(\"CA\", \"FL\", \"MA\", \"NV\", \"NM\", \"NY\", \"OH\", \"TX\", \"WA\", \"WY\"))) +\n  geom_point(aes(x = state, y = reported_date_time, color = reported_date_time, alpha= .5))+\n  labs(\n    x = \"State\",\n    y = \"Time\"\n  )"
  },
  {
    "objectID": "project3.html",
    "href": "project3.html",
    "title": "Project 3: A simulation study",
    "section": "",
    "text": "Description of Project\nEven though I just turned 20 this October, I believe a lot of people are complexed by casino games and its probabilities when it comes to winning. How likely are you to win at a game of poker, a game of roulette, or even a game of blackjack? Are any of these games skill based or in “your” favor? What game should I go and play if I want to earn money fast? Now I am not going to go gamble any time soon, not only because I am underage but also know better then to fall in that rabbit hole, but I really want to learn more about probabilities when it comes to playing such games and putting one’s money the line. I have heard a lot about the game blackjack and would want to test the probability of getting a value higher than or equal to 17 when drawing two cards because apparently you don’t want to hit if you are higher than 17. I am going to get this probability by running 10,000 simulations of 1)creating a deck with all amounts of cards, 2) have a variable have randomly chosen two values from such deck and 3) getting a logical value if the sum is between 17 and 21 (true) or not (false). I will also vary the amount of cards someone draws to see a range of probabilities\n\n\nCode\n1st step: Creates a deck containing 4 values of 2-10, 4 values of 10, and 4 values of 11. Problem: Can’t tell which are diamonds, spades, clovers, or hearts however in this instance we only care about the value so this works. The deck is also irreplacable when you take a card out.\n\nlibrary(tidyverse)\n\nnewdeck &lt;- function() {\n  deck &lt;- c(rep(2:10, each = 4), rep(10, 4*3), rep(11, 4))  \n  return(deck)\n}\n\n\ndeck &lt;- newdeck()\ndeck\n\n [1]  2  2  2  2  3  3  3  3  4  4  4  4  5  5  5  5  6  6  6  6  7  7  7  7  8\n[26]  8  8  8  9  9  9  9 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 11 11\n[51] 11 11\n\n\n\nAdd values we drew, also has a while loop that changes aces from 11 to 1 if we are above 21.\n\n\ncalculate_value &lt;- function(n) {\n  total &lt;- sum(n)\n  num_aces &lt;- sum(n == 1)\n  while(total &gt; 21 && num_aces &gt;0){\n    total &lt;- total - 10\n    num_aces &lt;- num_aces -1\n  }\n  return(total)\n}\n\n\nRuns one simulation of drawing two cards from the deck and returns true or false if it is a high value (17-21)\n\n\nbetween17and21 &lt;- function(num_cards) {\n  deck &lt;- newdeck()\n  hand &lt;- sample(deck, num_cards, replace = FALSE)\n  hand_value &lt;- calculate_value(hand)\n  return(hand_value &gt;= 17 && hand_value &lt;= 21)\n}\n\n4.Run simulation multiple times and right now we are only testing 2 cards drawn.\n\ndraw_cards &lt;- function(nums,reps){\n  probability &lt;- map_lgl(1:reps, ~between17and21(nums)) |&gt; mean()\n  return(data.frame(high_hand_prob = probability, amount_of_cards = nums))\n}\n\n\nset.seed(47)\ndraw_cards(2,10000)\n\n  high_hand_prob amount_of_cards\n1         0.3474               2\n\n\n\n\nVisualizations\nIterate it from drawing 2 to 10 cards and Visualization: This graph represents a line graph of probabilities of a hand value being between 17-21 based on the amount of cards we draw. It has a downward trend which makes sense however we can see that at 2-3 cards we have a probability around 30% which is pretty high considering that we want to be in such a high range when playing this game. The cut off is around 6 cards where it is almost impossible to be at such a value.\n\nset.seed(47)\nmap(2:10, ~draw_cards(.x, reps = 10000)) |&gt; \n  list_rbind()|&gt;\n  ggplot(aes(x= amount_of_cards, y = high_hand_prob)) +\n  geom_line() +\n  labs(title = \"Probability of having a high hand value (17-21) in BlackJack\" ,y = \"probability of having a hand value of 17-21\", x = \"number of cards drawn\")\n\n\n\n\n\n\n\n\nMid-way I also wanted to calculate the chances of staying below 21 while drawing cards using similar functions with some altercations:\n\nbetween0and21 &lt;- function(num_cards) {\n  deck &lt;- newdeck()\n  hand &lt;- sample(deck, num_cards, replace = FALSE)\n  hand_value &lt;- calculate_value(hand)\n  return(hand_value &gt;= 0 && hand_value &lt;= 21)\n}\n\n\ndraw_cards_alt &lt;- function(nums,reps){\n  probability &lt;- map_lgl(1:reps, ~between0and21(nums)) |&gt; mean()\n  return(data.frame(high_hand_prob = probability, amount_of_cards = nums))\n}\n\nIn this graph we see a representation of probabilities of staying under or at 21 when drawing cards. Similarly, we see another downward trend and a cut off at 6 cards since again it is impossible to be within 21 at such amount of cards. The line has a very straight slope and suggests that probability gets worse the more cards we get which makes sense as the deck of cards has more high value cards than lower values.\n\nset.seed(47)\nmap(2:10, ~draw_cards_alt(.x, reps = 10000)) |&gt; \n  list_rbind()|&gt;\n  ggplot(aes(x= amount_of_cards, y = high_hand_prob)) +\n  geom_line() +\n  labs(title = \"Probability of not going above 21 in Blackjack\" ,y = \"probability of having a hand value of 0-21\", x = \"number of cards drawn\")\n\n\n\n\n\n\n\n\n\n\nConclusion\nOverall my main goal was to calculate the chances of a player getting a good hand at the start of a blackjack game and wanted to continue seeing the probability of staying in that range throughout the game. I did this by having a simulation done 10000 times for if a player draws 2 cards all the way to 10 cards and see the probabilities between these choices. Throughout the code I explain what each one does. We can see such probabilities in the first plot and can conclude we have a high chance of getting a good hand in the first two cards and from there the probability gets worse as you play the game. This isn’t necessarily saying you have only a 30 percent chance to have a good hand in the game but it suggests that you can anticipate to win based on the lower amount of cards you have. This is also connected to the second graph calculating probabilities of staying under 21 based on the amount of cards you have. Both graphs have a similar trend and suggest the conclusion that you want to stay within a 3-4 card range when playing the game. To conclude, when playing blackjack you have the highest chance of winning (having a good hand) when starting the game and the more you hit (draw a card) the less chance you have at winning. Blackjack is a very probabilistic game as in this simulation I don’t take into account the decision of hitting or staying based on the dealers hand, since if the dealer has a bad hand, aka a hand that most likely will break 21, you don’t want to worsen your chances since you want to rely on them breaking first. I would love to continue running a simulation like this however it seems very complex and could get confusing but I am happy I got these results in the end."
  },
  {
    "objectID": "projecttwo.html",
    "href": "projecttwo.html",
    "title": "Project 2 Analysis",
    "section": "",
    "text": "My overall goal is to analyze this Netflix dataset is to: 1. Detect a word in a description, such as family, love, and friend, and compare how many times it appears in certain genres and compare it with others. 2. See how long descriptions are in comparison to movies/shows that have actors that start with T or A followed with a vowel to those that don’t.\nHopefully these graphs can show something useful but due to this dataset being only around ~8000 rows and my observations being very specific may lead to little to no results."
  },
  {
    "objectID": "projecttwo.html#final-visuals",
    "href": "projecttwo.html#final-visuals",
    "title": "Project 2 Analysis",
    "section": "Final Visuals",
    "text": "Final Visuals\nI created three graphs located below answering my questions at the beginning.\n\nCount of Word in Netflix Movie Descriptions by Genre\nIn this data set, about movies, we can see that the word “love” is mentioned the most in the description of comedies and least in Action & Adventure. Friend is mentioned the same amount of times throughout the three genres. Family is mentioned the most in Comedies and Dramas and the least in comedies.\n\nlibrary(ggplot2)\nlibrary(tidyr)\nlibrary(dplyr)\nggplot(netflix_movies_filter, aes(fill = keyword, y = count, x = listed_in)) + \n  geom_bar(position = \"dodge\", stat = \"identity\") +  \n  labs(title = \"Count of Word in Netflix Movie Descriptions by Genre\",\n       x = \"Genre\",\n       y = \"Count of Keywords\",\n       fill = \"Keyword\") +  \n  theme_minimal()  \n\n\n\n\n\n\n\n\n\n\nCount of Word in Netflix Shows Descriptions by Genre\nIn this dataset, about shows, the most insightful thing we see is that love is mentioned the most in Dramas and comedies in comparison to Action & Adventure.\n\nlibrary(ggplot2)\nlibrary(tidyr)\nlibrary(dplyr)\nggplot(netflix_shows_filter, aes(fill = keyword, y = count, x = listed_in)) + \n  geom_bar(position = \"dodge\", stat = \"identity\") +  # Grouped bars\n  labs(title = \"Count of Word in Netflix TV Shows Descriptions by Genre\",\n       x = \"Genre\",\n       y = \"Count of Keywords\",\n       fill = \"Keyword\") + \n  theme_minimal()  \n\n\n\n\n\n\n\n\n\n\nDescription Length in Comparison with Actors starting with T or A\nSince the boxplot isn’t that different between each other, we can make grand observation. One thing to note is that cast labeled as N/A has very short descriptions.\n\nlibrary(ggplot2)\n\nggplot(netflix_titles, aes(x = as.factor(actor_TA), y = description_length, fill = as.factor(actor_TA))) +\n  geom_boxplot() +\n  labs(title = \"Description Length in Comparison with Actors starting with T or A\",\n       x = \"Contains 'T' or 'A' in Cast\",\n       y = \"Description Length\",\n       fill = \"Contains 'T' or 'A' \") +\n  scale_x_discrete(labels = c(\"No\", \"Yes\")) + \n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nReflection\nOverall, I feel like I can gain more insight if I use multiple words and more genres. The word “love”, “family”, and “friend” is very specific and that makes it so our histograms are so small. It would have been better if I used more generic words or used a variety of words. On the counterpart, when looking at actors that start with T or A, I need to be more specific so I can see a difference between the two groups.\n\n\nReference\nI got my data set from the tidy tuesday website/section on github. Found at:\nhttps://github.com/rfordatascience/tidytuesday/blob/master/data/2021/2021-04-20/readme.md"
  }
]